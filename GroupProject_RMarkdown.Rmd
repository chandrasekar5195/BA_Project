---
title: "MGMT655 Group Project Submission"
subtitle: "Data Science Project for MGMT 655 Business Analytics for Decision Making"
author: "Na HUANG, Emmanuel RICHARD, Chandrasekar VENKATARAMAN, Do Hoang Yen LE"
date: "26 July 2022"
output:
  html_document:
    prettydoc::html_pretty:
      theme: Cayman
    highlight: breezedark
    # css: styles.css
    # latex_engine: xelatex
    # mainfont: Calibri Light
    toc: yes
    toc_float: 
      collapsed: false
      smooth_scroll: true
    number_sections: false
---

```{r setup, include=F}
# Global Setting
knitr::opts_chunk$set(echo = T, 
                      warning = F, 
                      message = F,
                      cache = T,
                      dpi = 600, 
                      fig.width = 10, 
                      fig.height = 6, 
                      fig.align = "center")
```

```{css, echo = F}
h1 { color: rgb(62, 6, 148); }
h2 { color: rgb(0, 104, 139); } 
h3 { color: rgb(51, 122, 183); }

body {font-family:  -apple-system, BlinkMacSystemFont, 
                    "Segoe UI", Roboto, Ubuntu;
      font-size: 12pt; }

code { color: rgb(205,79,57) }

.tocify-extend-page {height: 0 !important; }
```

## 1.Business Question

> Lorem ipsum dolor sit amet, consectetur adipiscing elit. Morbi interdum dui non nisl molestie, ac tempus neque suscipit. Curabitur et elementum sapien. Duis id justo at nibh feugiat eleifend. Mauris non dui imperdiet, placerat nulla non, finibus velit. Praesent diam orci, fringilla quis nulla eu, commodo congue ipsum. Donec erat felis, venenatis eu nisi eu, dictum egestas erat. Sed quis tempus lectus. Vestibulum lobortis tellus et ullamcorper lacinia. Fusce faucibus porttitor enim, ultrices egestas eros mattis ac. Suspendisse at maximus sapien. Aenean faucibus augue ut congue ornare. Quisque tristique ex eget eros fermentum condimentum. Vivamus ut mi leo. Aenean imperdiet, felis sed ultricies porttitor, lectus sem congue diam, ut malesuada sem turpis sed arcu. Proin tincidunt ultricies est id sollicitudin. Suspendisse potenti.

> Lorem ipsum dolor sit amet, consectetur adipiscing elit. Morbi interdum dui non nisl molestie, ac tempus neque suscipit. Curabitur et elementum sapien. Duis id justo at nibh feugiat eleifend. Mauris non dui imperdiet, placerat nulla non, finibus velit. Praesent diam orci, fringilla quis nulla eu, commodo congue ipsum. Donec erat felis, venenatis eu nisi eu, dictum egestas erat. Sed quis tempus lectus. Vestibulum lobortis tellus et ullamcorper lacinia. Fusce faucibus porttitor enim, ultrices egestas eros mattis ac. Suspendisse at maximus sapien. Aenean faucibus augue ut congue ornare. Quisque tristique ex eget eros fermentum condimentum. Vivamus ut mi leo. Aenean imperdiet, felis sed ultricies porttitor, lectus sem congue diam, ut malesuada sem turpis sed arcu. Proin tincidunt ultricies est id sollicitudin. Suspendisse potenti.  

> Lorem ipsum dolor sit amet, consectetur adipiscing elit. Morbi interdum dui non nisl molestie, ac tempus neque suscipit. Curabitur et elementum sapien. Duis id justo at nibh feugiat eleifend. Mauris non dui imperdiet, placerat nulla non, finibus velit. Praesent diam orci, fringilla quis nulla eu, commodo congue ipsum. Donec erat felis, venenatis eu nisi eu, dictum egestas erat. Sed quis tempus lectus. Vestibulum lobortis tellus et ullamcorper lacinia. 

>  Lorem ipsum dolor sit amet, consectetur adipiscing elit. Morbi interdum dui non nisl molestie, ac tempus neque suscipit. Curabitur et elementum sapien. Duis id justo at nibh feugiat eleifend. Mauris non dui imperdiet, placerat nulla non, finibus velit. Praesent diam orci, fringilla quis nulla eu, commodo congue ipsum. Donec erat felis, venenatis eu nisi eu, dictum egestas erat. Sed quis tempus lectus. Vestibulum lobortis tellus et ullamcorper lacinia. Fusce faucibus porttitor enim, ultrices egestas eros mattis ac. Suspendisse at maximus sapien. Aenean faucibus augue ut congue ornare. Quisque tristique ex eget eros fermentum condimentum. 

## 2. Import

> Load packages

```{r Load Package}
pacman::p_load(tidyverse, lubridate, tidymodels, skimr, 
               GGally, ggstatsplot, Hmisc, broom, plotly, 
               DT, doParallel, parsnip, themis, ranger,
               ggpubr, vip, ggthemes, ggthemr
)
#source("Functions.R")
```

> Data was sourced from [Kaggle](https://www.kaggle.com/datasets/imsparsh/churn-risk-rate-hackerearth-ml)

Variable (Feature) Name    | Description
:--------------------------|:--------------------------------------------------------------------
`churn`                   | `Yes` if the patient had a stroke or `No` if not
points_in_wallet          | Points in wallet of the customer
membership_category       | "No Membership", "Basic Membership", "Premium Membership", "Silver                              Membership", "Gold Membership", "Platinum Membership"
avg_transaction_value     | Average transaction value of the customer
avg_time_spent            | Average time spent on the website by the customer
avg_with_company          | Time since the joining date of the customer
age                       | Age of the customer
avg_frequency_login_days_interval  | Average frequency the customer logs in       ("0-10","10-20","20-30",">30")             

```{r Import Data}
churn_df <- read_csv("churn_dataset_train.csv")
```

```{r Data table}
churn_df %>% 
  datatable(options = list(scrollX = T))
```  

## 3. Transform & EDA

```{r}
skim(churn_df) 
```

```{r Transform Data}
churn_cleaned <- churn_df %>%
  filter(days_since_last_login > 0 & avg_time_spent > 0 & avg_frequency_login_days > 0
         & avg_frequency_login_days != "Error" & gender != "Unknown"
         & joined_through_referral != "?" & churn_risk_score > 0) %>%
  drop_na() %>%
  mutate(age_with_company = difftime(Sys.Date(),joining_date, units = "days")) %>%
  mutate(avg_frequency_login_days_interval =
           ifelse(as.numeric(avg_frequency_login_days) > 0 & as.numeric(avg_frequency_login_days) <= 10, 1,
                  ifelse(as.numeric(avg_frequency_login_days) > 10 & as.numeric(avg_frequency_login_days) <= 20, 2,
                         ifelse(as.numeric(avg_frequency_login_days) > 20 & as.numeric(avg_frequency_login_days) <= 30, 3,
                                ifelse(as.numeric(avg_frequency_login_days) > 30, 4, 0)
                         )))) %>%
  mutate(membership_category = ifelse(membership_category == "Premium "))
  select(-medium_of_operation,
         -referral_id, -customer_id, -security_no, -Name) %>%
  mutate(churn = ifelse(churn_risk_score >=5, "Yes", "No")) %>%
  complete() %>%
  dplyr::mutate_all(as.factor) %>%
  dplyr::mutate(across(c(
    age, avg_time_spent, points_in_wallet,
    age_with_company, avg_transaction_value,
  avg_frequency_login_days
  ),as.numeric)) %>%
  mutate(churn1 = fct_relevel(churn, "Yes"))

churn_cleaned <- churn_cleaned %>% 
  select(churn, points_in_wallet, membership_category, avg_transaction_value,
         avg_time_spent, age_with_company, age, avg_frequency_login_days_interval)
churn_cleaned <- churn_cleaned[sample(nrow(churn_cleaned),1000),]
```


```{r Check Transform Data}
skim(churn_cleaned)
```

### 3.1 Uni-variate Analysis {.tabset}

> Key demographic variables are analysed to determine the characteristics of patients in the sample. 

#### 3.1.1 Churn

> Data set is highly imbalanced because stroke is a rare event. Balancing is required before applying Machine Learning Algorithm.

```{r Churn Count}
ggthemr("fresh")

churn_cleaned %>%
  mutate(churn = recode(churn, "Yes" = "Churn", "No" = "Not Churn")) %>%
  ggplot(aes(churn)) +
  geom_bar(aes(y = (..count..)/sum(..count..))) +
  scale_y_continuous(labels = percent) +
  geom_label(aes(label = percent((..count..)/sum(..count..)),
                y = (..count..)/sum(..count..)), 
            stat = "count",
            size = 5,
            fill = "white") +
  labs(title = "Proportion of Churned Customers",
       x = "Churn",
       y = "Proportion of Customers")
```

#### 3.1.2 Age

> Age covers full range. Average customer age is 28 years-old.

```{r Age Distribution}
# should we have a cut off for age?
churn_cleaned %>%
  ggplot() +
  geom_density(aes(x = age)) +
  geom_vline(aes(xintercept = mean(age)),
             linetype = "dashed",
             color = "tomato3") +
  annotate(geom = "text",
         label = "Mean = 28",
         color = "tomato3",
         x = 28, 
         y = 0.017) +
  labs(title = "Age Distribution of Customers",
       y = "Density",
       x = "Age")

summary(churn_cleaned$age)
```

#### 3.1.3 Points in wallet

> Lorem ipsum dolor sit amet, consectetur adipiscing elit. Morbi interdum dui non nisl molestie, ac tempus neque suscipit. Curabitur et elementum sapien. Duis id justo at nibh feugiat eleifend.

```{r Points in wallet Distribution}
churn_cleaned %>%
  ggplot() +
  geom_density(aes(x = points_in_wallet)) +
  geom_vline(aes(xintercept = mean(points_in_wallet)),
             linetype = "dashed",
             color = "tomato3") +
  labs(title = "Points in wallet distribution of customers",
       subtitle = "<Will fill in something here>",
       y = "Density",
       x = "Points in wallet")
```

#### 3.1.4 Average time spent

> Lorem ipsum dolor sit amet, consectetur adipiscing elit. Morbi interdum dui non nisl molestie, ac tempus neque suscipit. Curabitur et elementum sapien.

```{r Average time spent Distribution}
churn_cleaned %>%
  ggplot() +
  geom_density(aes(x = avg_time_spent)) +
  geom_vline(aes(xintercept = mean(avg_time_spent)),
             linetype = "dashed",
             color = "tomato3") +
  labs(title = "Average time spent by Customers",
       y = "Density",
       x = "Average time spent")

summary(churn_cleaned$avg_time_spent)
```


### 3.2 Multi-variate Analysis {.tabset}

> Lorem ipsum dolor sit amet, consectetur adipiscing elit. Morbi interdum dui non nisl molestie, ac tempus neque suscipit. Curabitur et elementum sapien. Duis id justo at nibh feugiat eleifend. Mauris non dui imperdiet, placerat nulla non, finibus velit. Praesent diam orci, fringilla quis nulla eu, commodo congue ipsum. 

> Lorem ipsum dolor sit amet, consectetur adipiscing elit. Morbi interdum dui non nisl molestie, ac tempus neque suscipit. 

#### 3.2.1 Membership category

> Lorem ipsum dolor sit amet, consectetur adipiscing elit. Morbi interdum dui non nisl molestie, ac tempus neque suscipit. 

```{r Membership Category}
churn_cleaned %>%
  mutate(stroke = recode(churn, "Yes" = "Churn", "No" = "Not churn")) %>%
  ggplot(aes(churn, group = membership_category)) +
  geom_bar(aes
           (y = ..prop.., 
             fill = factor(..x..)), 
           stat = "count", 
           show.legend = FALSE) +
  geom_label(aes(label = percent(..prop..),
                y = ..prop..), 
            stat = "count",
            size = 5,
            fill = "white") +
  labs(title = "Proportion of churn/not churn With Membership Category",
       x = "Membership Category", 
       y = "Percentage", 
       fill = "membership_category") +
  facet_wrap(~membership_category) +
  scale_y_continuous(labels = percent)  +
  coord_flip()
```
#### 3.2.2 Average Frequency of login days

> Lorem ipsum dolor sit amet, consectetur adipiscing elit. Morbi interdum dui non nisl molestie, ac tempus neque suscipit. 

```{r Average Frequency of Login days}
churn_cleaned %>%
  mutate(churn = recode(churn, "Yes" = "Churn", "No" = "Not churn")) %>%
  mutate(avg_frequency_login_days_interval=recode(
    avg_frequency_login_days_interval,
    "1"="<=10 days",
    "2"="10-20 days",
    "3"="20-30 days",
    "4"=">30 days"
  )) %>% 
  ggplot(aes(churn, group = avg_frequency_login_days_interval)) +
  geom_bar(aes
           (y = ..prop.., 
             fill = factor(..x..)), 
           stat = "count", 
           show.legend = FALSE) +
  geom_label(aes(label = percent(..prop..),
                y = ..prop..), 
            stat = "count",
            size = 5,
            fill = "white") +
  labs(title = "Proportion of churn/not churn With average frequency of login days",
       x = "Avg Frequency of login days", 
       y = "Percentage", 
       fill = "avg_frequency_login_days_interval") +
  facet_wrap(~avg_frequency_login_days_interval) +
  scale_y_continuous(labels = percent) +
  scale_x_discrete(labels = c("Yes" = "Churn", "No" = "Not Churn")) +
  coord_flip()
```

#### 3.2.3 Average transaction value

> Lorem ipsum dolor sit amet, consectetur adipiscing elit. Morbi interdum dui non nisl molestie, ac tempus neque suscipit. 

```{r Smoke}
churn_cleaned %>%
  mutate(churn = recode(churn, "Yes" = "Churn", "No" = "Not Churn")) %>%
  ggplot(aes(membership_category, group = churn)) +
  geom_density(aes
           (avg_transaction_value, color = churn)) +
   labs(title = "Avg transaction value with Membership category",
       x = "Membership category", 
       y = "Avg Transaction value", 
       fill = "membership_category") +
  facet_wrap(~membership_category) 
```


#### 3.2.5 points in wallet & Stroke

> Customers who do not churn have higher average points in wallet, but effect size is small. 

```{r Points in wallet effect size}
churn_cleaned %>% 
  mutate(churn = recode(churn, "Yes" = "Churn", "No" = "Not Churn")) %>%
  ggbetweenstats(
    x = churn,
    y = points_in_wallet,
    plot.type = "box") 
```

#### 3.2.6 Average transaction value level

> Lorem ipsum dolor sit amet, consectetur adipiscing elit. Morbi interdum dui non nisl molestie, ac tempus neque suscipit. Curabitur et elementum sapien.

```{r Average Transaction value  Size Across membership category}
churn_cleaned %>%
  mutate(churn = recode(churn, "Yes" = "Churn", "No" = "Not Churn")) %>% 
    ggplot(
    aes(x = churn,
    y = avg_transaction_value,group = churn,
    fill = membership_category)) +
    geom_boxplot() +
  facet_wrap(~membership_category, ncol = 1) +
  coord_flip()
```

#### 3.2.7 Age with company & churn

> Lorem ipsum dolor sit amet, consectetur adipiscing elit. Morbi interdum dui non nisl molestie, ac tempus neque suscipit. Curabitur et elementum sapien

```{r Age}
churn_cleaned %>% 
  mutate(churn = recode(churn, "Yes" = "Churn", "0" = "Not Churn")) %>%
  ggbetweenstats(
    x = churn,
    y = age_with_company,
    plot.type = "box")
```

### 3.3 Correlation Study {.tabset}

> Prep and bake. Since hte classes of variables have already been assigned properly in the cleaning stage, we go with normalizing all numeric predictors and dummyfying all nominal predictors.

```{r Reciped for EDA}
library(themis)

reciped_EDA <- recipe(formula = churn ~ .,
                          data = churn_cleaned) %>%
  step_normalize(all_numeric_predictors()) %>% 
  step_dummy(churn, all_nominal_predictors())

baked_EDA <- reciped_EDA %>%
  prep(retain = TRUE) %>%
  bake(new_data = NULL)
```


```{r}
baked_EDA %>% count(churn)
```

> Lorem ipsum dolor sit amet, consectetur adipiscing elit. Morbi interdum dui non nisl molestie, ac tempus neque suscipit. Curabitur et elementum sapien.

#### 3.3.1 Scatter Plot 

```{r Scatter PLot}
baked_EDA %>%
  select(churn, points_in_wallet, age, avg_time_spent) %>%
  ggpairs()
```

#### 3.3.2 Correlation Matrix

```{r Correlation Matrix}
baked_EDA %>% 
  mutate(churn = as.numeric(churn),
         churn = ifelse(churn == "Yes", 1, 0)
         ) %>% 
  select(churn, where(is.numeric)) %>% 
  as.matrix(.) %>% 
  rcorr(.) %>% 
  tidy(.) %>% 
  mutate(absolute_corr = abs(estimate)
  ) %>% 
  rename(variable1 = column1,
         variable2 = column2,
         corr = estimate) %>% 
  filter(variable1 == "churn" | variable2 == "churn") %>% 
  datatable()
```

## 4. Predictive Model

> Lorem ipsum dolor sit amet, consectetur adipiscing elit. Morbi interdum dui non nisl molestie, ac tempus neque suscipit. Curabitur et elementum sapien.

> Lorem ipsum dolor sit amet, consectetur adipiscing elit. Morbi interdum dui non nisl molestie, ac tempus neque suscipit. Curabitur et elementum sapien.

> Lorem ipsum dolor sit amet, consectetur adipiscing elit. Morbi interdum dui non nisl molestie, ac tempus neque suscipit. Curabitur et elementum sapien.

### 4.1. Split

> Preparation
> 80% of the samples have been considered for training and 20% of the samples have been considered for testing/validating. 

```{r}

set.seed(100)
churn_split <- churn_cleaned %>% 
  initial_split(prop = 0.8, strata = churn)
```

> Execution

```{r}
churn_train <- churn_split %>% training() 
churn_test <- churn_split %>% testing()
```

```{r}
### 4.1. Functions for faster code development 
workflow_generator <- function(var_recipe, var_model)
{
    workflow() %>% 
    add_recipe({var_recipe}) %>% 
    add_model({var_model})
}

perf_and_pred_generator <- function(var_workflow, var_tune, var_split)
{
  var_parameters <- {var_tune} %>% 
    select_best(metric = "roc_auc")
  
  var_finalized_workflow <- {var_workflow} %>% 
    finalize_workflow(var_parameters)
  
  var_fit <- var_finalized_workflow %>% 
    last_fit(var_split)
  
  var_performance <- var_fit %>% 
    collect_metrics()
  
  var_predictions <- var_fit %>% 
    collect_predictions()
  
  return(list(perf = var_performance, pred = var_predictions))
  
}

CM_builder <- function(data, outcome, title_name)
{ 
  {data} %>% 
    conf_mat({outcome}, .pred_class) %>% 
    pluck(1) %>% 
    as_tibble() %>% 
    mutate(cm_colors = ifelse(Truth == "Yes" & Prediction == "Yes", "True Positive",
                              ifelse(Truth == "Yes" & Prediction == "No", "False Negative",
                                     ifelse(Truth == "No" & Prediction == "Yes", 
                                            "False Positive", 
                                            "True Negative")
                              )
    )
    ) %>% 
    ggplot(aes(x = Prediction, y = Truth)) + 
    geom_tile(aes(fill = cm_colors), show.legend = F) +
    scale_fill_manual(values = c("True Positive" = "green",
                                 "False Negative" = "red",
                                 "False Positive" = "red",
                                 "True Negative" = "green")
    ) + 
    geom_text(aes(label = n), color = "white", size = 10) + 
    geom_label(aes(label = cm_colors), vjust = 2
    ) + 
    theme_fivethirtyeight() + 
    theme(axis.title = element_text()
    )  + 
   labs(title = {title_name})
}

feature_importance_extractor <- function(workflow_data, full_dataset)
{
  finalized_model <- {workflow_data} %>% fit({full_dataset})
  
  model_summary <- pull_workflow_fit(finalized_model)$fit
  
  feature_importance <- data.frame(importance = model_summary$variable.importance) %>% 
    rownames_to_column("feature") %>% 
    as_tibble() %>% 
    mutate(feature = as.factor(feature)) 
  
  feature_importance %>% 
    ggplot(aes(x = importance, y = reorder(feature, importance), fill = importance)) +
    geom_col(show.legend = F) +
    scale_fill_gradient(low = "deepskyblue1", high = "deepskyblue4") +
    scale_x_continuous(expand = c(0, 0)) +
    labs(
      y = NULL,
      title = "Feature (Variable) Importance for Churn Prediction") + 
    ggthemes::theme_fivethirtyeight()
}
```
### 4.2. Pre-Process

> Feature Engineering

```{r}
## Recipes ----
recipe_common <- recipe(churn ~ age + avg_time_spent + points_in_wallet + 
                          age_with_company + avg_transaction_value +
                          membership_category + 
                          avg_frequency_login_days_interval,
                        data = churn_train)

recipe_norm_dummy <- 
  recipe_common %>% 
  step_normalize(all_numeric_predictors()) %>% 
  step_dummy(all_nominal_predictors())


recipe_boxcox <- 
  recipe_common %>% 
  step_BoxCox(all_numeric_predictors()) %>% 
  step_normalize(all_numeric_predictors()) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_interact(terms = ~ avg_transaction_value:starts_with("membership_category")) %>% 
  step_poly(avg_transaction_value, degree = 2, role = "predictor") %>% 
  step_poly(points_in_wallet, degree = 2, role = "predictor")

  
recipe_up <-   
  recipe_common %>% 
  step_upsample(churn) %>% 
  step_normalize(all_numeric_predictors()) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_interact(terms = ~ avg_transaction_value:starts_with("membership_category")) %>% 
  step_poly(avg_transaction_value, degree = 2, role = "predictor") %>% 
  step_poly(points_in_wallet, degree = 2, role = "predictor")

```

### 4.3. Creating the model objects

```{r}
## Logistic Regression Model ----
model_glm <- 
  logistic_reg(mode = "classification") %>% 
  set_engine("glm") 
## RF Model ----
model_RF <- 
  rand_forest() %>% 
  set_args(mtry = tune()) %>% 
  set_engine("ranger", importance = "impurity") %>% 
  set_mode("classification")

## XG Boost Model ----
model_xgb <- 
  boost_tree(trees = 1000,
             mtry = tune(),
             min_n = tune(),
             tree_depth = tune(),
             sample_size = tune(),
             learn_rate = tune()
  ) %>% 
  set_engine("xgboost") %>% 
  set_mode("classification")

##NULL Model
model_null <- null_model() %>% 
  set_engine("parsnip") %>% 
  set_mode("classification")
```

### 4.4. Creating the workflow objects

> Using `workflows::``workflow()`

```{r}
workflow_null <- workflow_generator(recipe_norm_dummy,model_null)

## Logistic Regression 
workflow_glm <- workflow_generator(recipe_norm_dummy, model_glm)

## RF Model with upsampling
workflow_RF_up <- workflow_generator(recipe_up, model_RF)

## RF Model with BoxCox
workflow_RF_boxcox <- workflow_generator(recipe_boxcox, model_RF)


## XG Boost Model with upsampling
workflow_xg_up <- workflow_generator(recipe_up, model_xgb)

## XG Boost Model with BoxCox
workflow_xg_boxcox <- workflow_generator(recipe_boxcox, model_xgb)
```
### 4.5. Creating the cross validation sets and tuning parameters
> Cross-validation (cv)

```{r}
###Cross Validation
set.seed(100)
CV_10 <- churn_train %>% 
  vfold_cv(v = 10, strata = churn)

###Tuning Parameters
grid_RF <- expand.grid(mtry = c(3,4,5))

grid_XG <-
  grid_max_entropy(
    mtry(c(5L, 10L),
    ),
    min_n(c(10L, 40L)
    ),
    tree_depth(c(5L, 10L)
    ),
    sample_prop(c(0.5, 1.0)
    ),
    learn_rate(c(-2, -1)
    ),
    size = 20
  )
```

### 4.6. Tuning
> Parallel Processing (for Expedited/speedy Modeling)

```{r}
registerDoParallel()
### NULL
set.seed(100)
fit_null <- 
  workflow_null %>% 
  fit_resamples(CV_10,
                control = control_resamples(save_pred = T))

### Logistic Regressioin
set.seed(100)
tuned_glm <- 
  workflow_glm %>% 
  tune::tune_grid(resamples = CV_10,
                  metrics = metric_set(accuracy, roc_auc, f_meas)
  )
set.seed(100)
### Random Forrest with upsampling
tuned_RF_up <- workflow_RF_up %>% 
  tune::tune_grid(resamples = CV_10,
                  grid = grid_RF,
                  metrics = metric_set(accuracy, roc_auc, f_meas))
set.seed(100)
#### Random Forrest with BoxCox
tuned_RF_boxcox <- workflow_RF_boxcox %>% 
  tune::tune_grid(resamples = CV_10,
                  grid = grid_RF,
                  metrics = metric_set(accuracy, roc_auc, f_meas))
set.seed(100)
### XGBoost with upsampling
tuned_xg_up <- 
  workflow_xg_up %>% 
  tune_grid(resamples = CV_10,
            grid = grid_XG,
            control = control_grid(save_pred = T),
            metrics = metric_set(accuracy, roc_auc, f_meas)
  )

set.seed(100)
### XGBoost with BoxCox
tuned_xg_boxcox <- 
  workflow_xg_boxcox %>% 
  tune_grid(resamples = CV_10,
            grid = grid_XG,
            control = control_grid(save_pred = T),
            metrics = metric_set(accuracy, roc_auc, f_meas)
  )
```

### 4.7. Selecting best model and collecting predictions
```{r Best Model}
### NULL
performance_null <- fit_null %>% collect_metrics()
predictions_null <- fit_null %>% collect_predictions()


# Logisic Regression
perf_and_pred <- perf_and_pred_generator(workflow_glm, tuned_glm, churn_split)
performance_glm <- perf_and_pred$perf
predictions_glm <- perf_and_pred$pred

# Random Forrest with Upsampling 
perf_and_pred <- perf_and_pred_generator(workflow_RF_up, tuned_RF_up, churn_split)
performance_RF_up <- perf_and_pred$perf
predictions_RF_up <- perf_and_pred$pred

# Random Forrest with BoxCox
perf_and_pred <- perf_and_pred_generator(workflow_RF_boxcox, tuned_RF_boxcox, churn_split)
performance_RF_boxcox <- perf_and_pred$perf
predictions_RF_boxcox <- perf_and_pred$pred

# XGBoost with Upsampling
perf_and_pred <- perf_and_pred_generator(workflow_xg_up, tuned_xg_up, churn_split)
performance_xg_up <- perf_and_pred$perf
predictions_xg_up <- perf_and_pred$pred

# XGBoost with BoxCox
perf_and_pred <- perf_and_pred_generator(workflow_xg_boxcox, tuned_xg_boxcox, churn_split)
performance_xg_boxcox <- perf_and_pred$perf
predictions_xg_boxcox <- perf_and_pred$pred
```


> Comparing Models 

```{r Comparing 4 models}
predictions_null <- predictions_null %>% 
  mutate(model = "Null model")
predictions_glm <- predictions_glm %>% 
  mutate(model = "Logistic Regression")
predictions_RF_up <- predictions_RF_up %>% 
  mutate(model = "Random Forrest with Upsampling")
predictions_RF_boxcox <- predictions_RF_boxcox %>% 
  mutate(model = "Random Forrest with BoxCox")
predictions_xg_up <- predictions_xg_up %>% 
  mutate(model = "XGBoost with Upsampling")
predictions_xg_boxcox <- predictions_xg_boxcox %>% 
  mutate(model = "XGBoost with BoxCox")

performance_glm <- performance_glm %>% 
  mutate(model = "Logistic Regression")
performance_RF_up <- performance_RF_up %>% 
  mutate(model = "Random Forrest with Upsampling")
performance_RF_boxcox <- performance_RF_boxcox %>% 
  mutate(model = "Random Forrest with BoxCox")
performance_xg_up <- performance_xg_up %>% 
  mutate(model = "XGBoost with Upsampling")
performance_xg_boxcox <- performance_xg_boxcox %>% 
  mutate(model = "XGBoost with BoxCox")


comparing_models <- bind_rows(performance_glm,
          performance_RF_up,
          performance_RF_boxcox,
          performance_xg_up,
          performance_xg_boxcox) %>%
  select(-.estimator, -.config) %>%
  pivot_wider(names_from = .metric,
              values_from = .estimate) %>%
  datatable() %>%
  formatRound(columns = c("accuracy", "roc_auc"),
              digits = 2)
```

> Confusion Matrix

```{r}
CM_null <- CM_builder(predictions_null, "churn", predictions_null$model[1])
CM_glm <- CM_builder(predictions_glm, "churn", predictions_glm$model[1])
CM_RF_Up <- CM_builder(predictions_RF_up, "churn", predictions_RF_up$model[1])
CM_RF_boxcox <- CM_builder(predictions_RF_boxcox,"churn", predictions_RF_boxcox$model[1])
CM_xg_Up <- CM_builder(predictions_xg_up, "churn", predictions_xg_up$model[1])
CM_xg_boxcox <- CM_builder(predictions_xg_boxcox,"churn", predictions_xg_boxcox$model[1])
grid.arrange(CM_null, CM_glm,CM_RF_Up,CM_RF_boxcox,CM_xg_Up,CM_xg_boxcox)
```

> Null Model

```{r}
model_null <- null_model() %>% 
  set_engine("parsnip") %>% 
  set_mode("classification")

workflow_null <- workflow_generator(recipe_norm_dummy,model_null)

fit_null <- 
  workflow_null %>% 
  fit_resamples(CV_10,
                control = control_resamples(save_pred = T))


performance_null <- fit_null %>% collect_metrics()
predictions_null <- fit_null %>% collect_predictions()
```

> Comparing Model Performances

```{r}
comparing_predictions <- bind_rows(predictions_glm,
          predictions_RF_up,
          predictions_RF_boxcox,
          predictions_xg_up,
          predictions_xg_boxcox)
```

> ROC-AUC Curve

```{r}
comparing_predictions %>% 
  group_by(model) %>% 
  roc_curve(truth = churn,
            .pred_Yes) %>% 
  autoplot() +
  ggthemes::scale_color_wsj() +
  labs(title = "Comparisons of Predictive Power\nbetween models for Stroke",
       subtitle = "Random Forrest and XGBoost similar result but XGBoost with upsampling provides better recall",
       color = "Prediction Tools") +
  theme(legend.position = c(.65, .15))
```

> Feature Importance

```{r}
finalized_parameters <- tuned_xg_up %>% select_best(metric = "roc_auc")
finalized_workflow <- workflow_xg_up %>%finalize_workflow(finalized_parameters)
finalized_model <- finalized_workflow %>% fit(churn_cleaned)

finalized_model %>% 
  extract_fit_parsnip() %>% # pull_workflow_fit()
  vip(aesthetic = list(fill = "deepskyblue4",
                       alpha = 0.50)
      ) +
  labs(
    y = NULL,
    title = "Feature (Variable) Importance for Predicting Churn",
    subtitle = "Points in wallet has the highest importance")
```

> Deploy Machine Learning Algorithm to Dashboard

```{r}
finalized_model %>% saveRDS("finalized_churn_prediction_model.rds")
```

> Save R Data 

```{r}
save.image("GroupProject.RData")
```

## 5. Executive Summary

### 5.1. Evidence

* We tested 4 models for this predictive modeling task.

* Our first model`Random Forest ALgorithm A` up-sampled the minority class (non-stroke) and included all variables, excluding ID. 

* Based on our correlation analysis of all variables, **age**, **heart disease**, **average glucose level**, **hypertension** and **marriage** had the strongest impact on stroke. 

* Hence, the second model `Random Forest Algorithm B` focused on these 5 variables. An interaction between average glucose level, hypertension and heart disease was also factored into the recipe. 

* Based on variable importance graph, the variables "smoking_status" and "bmi" was included into `Random Forest Algorithm C` in an attempt to improve overall performance. 

* Based on `Random Forest Algorithm B`, we attempted to use the same variables with the computation method of XG Boost to improve the model performance.

* Of the 4 models, the fourth model `XG Boost Algorithm` performed better with higher roc_auc of **0.86** and higher accuracy of **0.90**, compared to the other three models.

* The top three variables that had the highest importance on the prediction results are **age**, followed by **hypertension** and **heart disease**.

### 5.2. Interpretation

* `XG Boost Algorithm` performed better overall with a roc_auc of **0.86** and accuracy of **0.90**, giving a 10% chance of error. 

* The model focused on the top 5 variables that have the largest effect on stroke based on the feature importance graph, and the relationship between average glucose level to hypertension and heart disease. 

* This implies the significance of focusing on the most important features during the predictive modelling task, and also research on interaction between variables to achieve optimal performance.

* In terms of variable importance, the top 3 variables are age, heart disease and hypertension. The effect size of these variables means `higher level of age, exisiting conditions of hypertension or heart disease` increases the chances of stroke detected. 

### 5.3. Recommendations

* The key use application for this predictive model is in determining whether a person is likely to be diagnosed with stroke based on their lifestyle and health measurements. Hence, the key stakeholders whom would most benefit from this predictive model are `(1) general practitioners`, `(2) patients and family members` and `(3) the public`. 

* This algorithm takes into account the 5 most important variables to produce a evidence-based prediction of the likelihood that a patient has stroke. For `general practitioners`, this predictive model is a useful data tool that can be used in conjunction with qualitative checks conducted based on their expertise. This would help them make **more accurate diagnosis** and recommend patients for further test (i.e. CT scan) and treatments. 

* In addition, this predictive model to **stratify patients into risk groups**. Without this tool, patients would belong to either stroke-positive or stroke-negative groups. However, with this predictive model that produces **quantitative likelihood of getting stroke**, general practitioners can derive a better understanding of the risk levels of a patient getting stroke (e.g. >50%) and thereby conduct necessary checks and provide pertinent advice on lifestyle plans to mitigate the onset of stroke.

* One of the drivers for this predictive model study was the [sharp rise in stroke cases in Singapore](https://www.straitstimes.com/singapore/8300-stroke-cases-admitted-to-public-hospitals-in-2018-public-outreach-campaign-starts) leading to health implications, which could have been avoided through early detection, so that lifestyle modifications and treatment can be provided.

* Hence, it is recommended that this prediction model be **made openly available to public to access and input their lifestyle factors and health measurements** to arrive at a predicted percentage likelihood of being stroke-positive. Singaporeans with higher likelihood of stroke (due to family history of stroke risk factors or lifestyle conditions) could also use this model to understand the risks so that they can make informed decisions on their next steps.

## Limitations

* This study applies to stroke only. 

* The results derived from this predictive model only indicates whether a person is likely to be stroke-positive or stroke-negative. It does not provide stroke diagnosis. 

* This model is easy and accessible for the public to use as it takes into account mainly lifestyle variables that are easily obtained. 

* However, the data set lacks other potential bio makers that are more salient in predicting stroke cases. Hence, further data collection and consultation with medical professionals must be pursued before this model can be applied to test for stroke. 

## References

* [Kaggle Data](https://www.kaggle.com/fedesoriano/stroke-prediction-dataset)

* [Stroke cases in Singapore](https://www.straitstimes.com/singapore/8300-stroke-cases-admitted-to-public-hospitals-in-2018-public-outreach-campaign-starts)

* [Stroke, how to prevent and what to look out for](https://www.straitstimes.com/singapore/how-do-i-prevent-stroke-and-what-should-i-look-out-for)

* [Singapore's stroke care system](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7399214/)

* [Stroke illiteracy in Singapore](https://annals.edu.sg/pdf/43VolNo9Sep2014/MemberOnly/V43N9p454.pdf)

* [Time window consideration for Thrombolysis](https://annals.edu.sg/pdf/43VolNo9Sep2014/MemberOnly/V43N9p454.pdf)

* [Preventive and early detection services](https://annals.edu.sg/pdf/43VolNo9Sep2014/MemberOnly/V43N9p454.pdf)

* [BMI interpretation](https://www.healthhub.sg/live-healthy/179/weight_putting_me_at_risk_of_health_problems)

* [Hedges' g interpretation](https://www.statisticshowto.com/hedges-g/)

## Appendix

* Nil.

## Contribution Statement

> The `pRoject` team put together by Prof. Roh (randomly) worked together to deliver the assignment. Together we defined the problem, identifed the data source and segregated tasks to deliver the final output.

  ***R-script** was led by `Teammate A`

  ***Shiny-dashboard** was led by `Teammate B`

  ***R-markdown** was led by `Teammate A & B`

> `Teammate A's` contributions:

1)	Collaborated with teammate to formulate the business problem. 

2)  Conducted research to discover interaction between variables.
  
2)	Created and developed the R-script – Transform & EDA, Split, Pre-process, Fit, Tune and Assess.
  
3)	Analyzed data to identify top variables that impact stroke.
  
4)	Designed recipes and tested to find the best model with highest roc_auc/accuracy score.
  
5)	Finalized the models and tested model on Dashboard. 
  
6)	Partnered with teammate to draft the Executive summary – Evidence, Interpretation and Recommendations.
  
7)	Created the pitch deck for the proposal and submit.
  
>  `Teammate B's` contributions:

1) Collaborated with team to develop the R-script – Transform & EDA, Split, Pre-process, Fit, Tune and Assess.

2) Collaborated to test and find the best recipe and model with highest roc_auc score.

3)  Debugged error messages preventing successful deployment of R-dashboard .
  
4)  Updated the dashboard according to our features to create the Stroke Prediction App.
 
5)  Partnered with team to draft the Rmarkdown file.

6)  Helped in writing the Executive Summary – Evidence, Interpretation and Recommendations.

<br>